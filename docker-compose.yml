x-common-settings: &common-settings
  image: ghcr.io/ggerganov/llama.cpp:server-cuda
  volumes:
    - ./cache:/root/.cache
  environment:
    TZ: Asia/Tokyo
  tty: true
  stop_grace_period: 0s
  ipc: host
  env_file:
    - .env

services:
  proxy:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - 4000:4000
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: --config /app/config.yaml
    restart: always
  main:
    <<: *common-settings
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              # device_ids: ['0', '1']
              capabilities: [ gpu ]
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo mmnga/Qwen3-30B-A3B-gguf
      --hf-file Qwen3-30B-A3B-Q4_K_M.gguf
      --threads 8
      --n-gpu-layers 99
      --ctx-size 32768
      -a gpt-4o-mini
  sub:
    <<: *common-settings
    ports:
      - "8001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [ gpu ]
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo unsloth/Qwen3-14B-GGUF
      --hf-file Qwen3-14B-Q4_K_M.gguf
      --threads 8
      --n-gpu-layers 99
      --ctx-size 4096
      -a gpt-4o-mini
  cpu:
    <<: *common-settings
    image: ghcr.io/ggerganov/llama.cpp:server
    ports:
      - "8002:8000"
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo mmnga/Qwen3-30B-A3B-gguf
      --hf-file Qwen3-30B-A3B-Q4_K_M.gguf
      --threads 8
      --ctx-size 32768
      -a gpt-4o-mini
