x-common-settings: &common-settings
  image: ghcr.io/ggerganov/llama.cpp:server-cuda
  volumes:
    - ./cache:/root/.cache
  environment:
    TZ: Asia/Tokyo
  tty: true
  stop_grace_period: 0s
  ipc: host
  env_file:
    - .env

services:
  proxy:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - 4000:4000
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: --config /app/config.yaml
    restart: always
  main:
    <<: *common-settings
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              # device_ids: ['0', '1']
              capabilities: [ gpu ]
      # 新URL対応
      # --hf-repo Aratako/calm3-22b-RP-GGUF
      # --hf-file calm3-22b-RP-Q4_K_M.gguf
      # --hf-repo grapevine-AI/Qwen2.5-32B-Instruct-GGUF-Japanese-imatrix
      # --hf-file qwen2.5-32b-instruct-Q4_K_M.gguf
      # --hf-repo unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF
      # --hf-file DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf
      # --hf-repo Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
      # --hf-file qwen2.5-coder-7b-instruct-q4_k_m.gguf
      # --hf-repo mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf
      # --hf-file ABEJA-Qwen2.5-32b-Japanese-v0.1-Q4_K_M.gguf
      # --hf-repo grapevine-AI/DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF
      # --hf-file DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf
      # --hf-repo lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF
      # --hf-file Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf
      # --hf-repo Undi95/Lumimaid-Magnum-v4-12B-GGUF
      # --hf-file Lumimaid-Magnum-v4-12B.q4_k_m.gguf
      # --hf-repo rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf
      # --hf-file deepseek-r1-distill-qwen2.5-bakeneko-32b-q4_k.gguf
      # --hf-repo Undi95/Lumimaid-Magnum-v4-12B-GGUF
      # --hf-file Lumimaid-Magnum-v4-12B.q8_0.gguf
      # --hf-repo bartowski/Qwen_QwQ-32B-GGUF
      # --hf-file Qwen_QwQ-32B-Q4_K_M.gguf
      # --hf-repo mmnga/Mistral-Small-3.1-24B-Instruct-2503-HF-gguf
      # --hf-file Mistral-Small-3.1-24B-Instruct-2503-HF-Q8_0.gguf
# 32768
      # --model /grpo/unsloth.Q8_0.gguf
# QwQ
# --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo mmnga/shisa-v2-qwen2.5-7b-gguf
      --hf-file shisa-v2-qwen2.5-7b-Q4_0.gguf
      --threads 8
      --n-gpu-layers 99
      --ctx-size 4096
      -a gpt-4o-mini
  sub:
    <<: *common-settings
    ports:
      - "8001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [ gpu ]
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo google/gemma-3-12b-it-qat-q4_0-gguf
      --hf-file gemma-3-12b-it-q4_0.gguf
      --threads 8
      --n-gpu-layers 99
      --ctx-size 4096
      -a gpt-4o-mini
