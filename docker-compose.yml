x-common-settings: &common-settings
  image: ghcr.io/ggml-org/llama.cpp:server-cuda
  volumes:
    - ./cache:/root/.cache
  environment:
    TZ: Asia/Tokyo
  tty: true
  stop_grace_period: 0s
  ipc: host
  env_file:
    - .env

services:
  proxy:
    image: ghcr.io/berriai/litellm:main-latest
    ports:
      - 4000:4000
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: --config /app/config.yaml
    restart: always
  main:
    <<: *common-settings
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              # device_ids: ['0', '1']
              capabilities: [ gpu ]
  # --hf-repo mmnga/Mistral-Small-3.1-24B-Instruct-2503-HF-gguf
  # --hf-file Mistral-Small-3.1-24B-Instruct-2503-HF-Q8_0.gguf
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF
      --hf-file Mistral-Small-3.2-24B-Instruct-2506-Q4_K_M.gguf
      --threads 4
      --n-gpu-layers 99
      --ctx-size 8096
      -a gpt-4o-mini --no-mmap
  sub:
    <<: *common-settings
    ports:
      - "8001:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['1']
              capabilities: [ gpu ]
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF
      --hf-file Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
      --threads 4
      --n-gpu-layers 30
      --ctx-size 4096
      -a gpt-4o-mini
      --jinja --temp 0.7 --min-p 0.0 --top-p 0.80 --top-k 20 --presence-penalty 1.0 

  cpu:
    <<: *common-settings
    image: ghcr.io/ggml-org/llama.cpp:server
    ports:
      - "8002:8000"
    command: >
      --host 0.0.0.0
      --port 8000
      --hf-repo unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF
      --hf-file Qwen3-30B-A3B-Instruct-2507-Q4_K_M.gguf
      --threads 8
      --ctx-size 4096
      -a gpt-4o-mini
