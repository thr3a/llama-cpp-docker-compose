vllm:
  <<: *common-settings
  build:
    context: ./vllm
    dockerfile: Dockerfile
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [ gpu ]
  ports:
    - "8002:8000"
  command: --model Qwen/Qwen2-VL-7B-Instruct-AWQ --quantization awq --served-model-name gpt-4o-mini

      # 新URL対応
      # --hf-repo Aratako/calm3-22b-RP-GGUF
      # --hf-file calm3-22b-RP-Q4_K_M.gguf
      # --hf-repo grapevine-AI/Qwen2.5-32B-Instruct-GGUF-Japanese-imatrix
      # --hf-file qwen2.5-32b-instruct-Q4_K_M.gguf
      # --hf-repo unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF
      # --hf-file DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf
      # --hf-repo Qwen/Qwen2.5-Coder-7B-Instruct-GGUF
      # --hf-file qwen2.5-coder-7b-instruct-q4_k_m.gguf
      # --hf-repo mmnga/ABEJA-Qwen2.5-32b-Japanese-v0.1-gguf
      # --hf-file ABEJA-Qwen2.5-32b-Japanese-v0.1-Q4_K_M.gguf
      # --hf-repo grapevine-AI/DeepSeek-R1-Distill-Qwen-32B-Japanese-GGUF
      # --hf-file DeepSeek-R1-Distill-Qwen-32B-Japanese-Q4_K_M.gguf
      # --hf-repo lmstudio-community/Qwen2.5-7B-Instruct-1M-GGUF
      # --hf-file Qwen2.5-7B-Instruct-1M-Q4_K_M.gguf
      # --hf-repo Undi95/Lumimaid-Magnum-v4-12B-GGUF
      # --hf-file Lumimaid-Magnum-v4-12B.q4_k_m.gguf
      # --hf-repo rinna/deepseek-r1-distill-qwen2.5-bakeneko-32b-gguf
      # --hf-file deepseek-r1-distill-qwen2.5-bakeneko-32b-q4_k.gguf
      # --hf-repo Undi95/Lumimaid-Magnum-v4-12B-GGUF
      # --hf-file Lumimaid-Magnum-v4-12B.q8_0.gguf
      # --hf-repo bartowski/Qwen_QwQ-32B-GGUF
      # --hf-file Qwen_QwQ-32B-Q4_K_M.gguf
      # --hf-repo mmnga/Mistral-Small-3.1-24B-Instruct-2503-HF-gguf
      # --hf-file Mistral-Small-3.1-24B-Instruct-2503-HF-Q8_0.gguf
# 32768
      # --model /grpo/unsloth.Q8_0.gguf
# QwQ
# --samplers "top_k;top_p;min_p;temperature;dry;typ_p;xtc"
