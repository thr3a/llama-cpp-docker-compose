FROM vllm/vllm-openai:latest

RUN pip install qwen-vl-utils
RUN pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.2.post1/flash_attn-2.7.2.post1+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl
# vllm-1  | WARNING 12-14 07:15:18 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
# vllm-1  | WARNING 12-14 07:15:18 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
# vllm-1  | WARNING 12-14 07:15:18 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
# vllm-1  | WARNING 12-14 07:15:18 utils.py:603] Current `vllm-flash-attn` has a bug inside vision module, so we use xformers backend instead. You can run `pip install flash-attn` to use flash-attention backend.
